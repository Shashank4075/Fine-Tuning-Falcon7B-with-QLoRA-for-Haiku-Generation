# -*- coding: utf-8 -*-
"""code_supplement_QLoRA_finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cz5PeHevwT1zxfZ6ky9pqNXtD8RwOdag

# Class 9 code supplement: Teaching Falcon7B to write better haikus using QLoRA

This notebook illustrates the process of performing paramater-efficient finetuning (PEFT) of the Falcon7B model using the a dataset of prompts that instruct the model to write haikus on different subjects.

## Step 1: Install needed libraries: `peft,` `bitsandbytes`, `accelerate`, and `trl`

The `peft` library contains Hugging Face implementations of various PEFT techniques, including LoRA. `bitsandbytes` will allow us to run Falcon in 4bit precision and `accelerate` has handy model loading utilities and also simplifies the process of finetuning in a distributed fashion. The `trl` library contains convenient classes for supervised finetuning.
"""

!pip install -q fsspec
!pip install -q accelerate
!pip install -q bitsandbytes
!pip install -q trl
!pip install -q peft
!pip install -q transformers
!pip install -q wandb

!pip install -q wandb

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from trl import SFTTrainer
import torch

"""## Step 2: Specify the Hugging Face model and the target modules
We'll be using Falcon7B for this task. Although this is a very low-performing model, it has the benefit of being fairly small and manageable. We also specify the target modules that we want to finetune in Falcon. Finetunable target modules differ by model type. For a list of common LLMs and their target modules see [here](https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220).
"""



model_name = "tiiuae/falcon-7b"
target_modules = ["query_key_value"]

"""## Step 3: Configure the 4-bit quantization logic and load the quantized model

To load the model, we need a configuration class that specifies how we want the quantization to be performed. We'll use `BitesAndBytesConfig` from the `Transformers` library for this. We are specifying the use of 4-bit quantization and also enabling double quantization to reduce the precision loss. We can then load the quantized model into memory.
"""

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

device_map = {"": 0}
foundation_model = AutoModelForCausalLM.from_pretrained(model_name,
                    quantization_config=bnb_config,
                    device_map=device_map,
                    use_cache = False)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

"""## Step 4: Examine output from the base (non-finetuned) model

The point of this exercise is to improve an LLM's ability to perform a particular task---in this case composing haikus. Let's first examine how Falcon does on this task without any finetuning.
"""

#this function returns the outputs from the model received, and inputs.
def get_outputs(model, inputs, max_new_tokens=100):
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        repetition_penalty=1.5, #Avoid repetition.
        early_stopping=False, #The model can stop before reach the max_length
        eos_token_id=tokenizer.eos_token_id,
    )
    return outputs

#Inference original model
input_sentences = tokenizer("Write a haiku that captures the beauty of new york city.", return_tensors="pt").to('cuda')
foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=100)

print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))

"""The LLM's haiku is quite bad--unimaginitive and full of repetitions. Let's teach it to be better!

## Step 5: Preparing the Dataset
Next, we pull a dataset of haiku prmpts from [here](https://huggingface.co/datasets/davanstrien/haiku_prompts/), load it, and take a 50-prompt sample.
"""

from datasets import load_dataset
dataset = "davanstrien/haiku_prompts"

#Create the Dataset to create prompts.
data = load_dataset(dataset)

data = data.map(lambda samples: tokenizer(samples["instructions"]), batched=True)
train_sample = data["train"].select(range(50))

del data
#train_sample = train_sample.remove_columns('tags')

display(train_sample)

print(train_sample[:10])

"""## Step 6: Fine-tuning
Finetuning starts with the creation of a LoRA configuration object containing the specified settings for finetuning,

* `r` This parameter specifies how many parameters will be trained. This can be tuned based on the complexity of the task.

* `lora_alpha` Adjust this to increase or decrease the weights of the LoRA activations. Larger values increases the impact of the finetuning data but too large values lead to catastrophic forgetting

* `lora_dropout` This is similar to the dropout regularization procedure used in general neural network training

* `bias` A rule of thumb for this parameter: For text classification the most common value is `none`, and for chat or question answering, `all` or `lora_only`

* `task_type` Indicates the task the model is being finetuned for. In this case, the task is text generation.
"""

import peft
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16, #larger values = more parameters to train.
    lora_alpha=16, # a scaling factor that adjusts the magnitude of the weight matrix
    target_modules=target_modules,
    lora_dropout=0.05, #Helps to avoid overfitting.
    bias="none", # this specifies if the bias parameter should be trained.
    task_type="CAUSAL_LM"
)

#Create a directory to contain the model
import os
working_dir = 'sample_data' #assumes you're on colab in the same directory as `sample_data
output_directory = os.path.join(working_dir, "peft_lab_outputs")

"""In the TrainingArgs we inform the number of epochs we want to train, the output directory and the learning_rate."""

import transformers
from transformers import TrainingArguments
training_args = TrainingArguments(
    output_dir=output_directory,
    auto_find_batch_size=True, # Find a correct baatch size that fits the size of Data.
    learning_rate= 2e-4, # Higher learning rate than full fine-tuning.
    num_train_epochs=5,
    report_to=None
)

"""Now we're ready to do our finetuning with the following elements:

* The model.
* The training_args
* The sataset
* The result of `DataCollator`, the dataset ready to be procesed in blocks.
* The LoRA config.
"""

import osgeo
tokenizer.pad_token = tokenizer.eos_token
# os.environ["WANDB_DISABLED"] = "true"

trainer = SFTTrainer(
    model=foundation_model,
    args=training_args,
    train_dataset=train_sample,
    peft_config = lora_config,
    tokenizer=tokenizer,
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)
)
trainer.train()

#Save the model.
peft_model_path = os.path.join(output_directory, f"lora_model")

#Save the model.
trainer.model.save_pretrained(peft_model_path)

#In case you are having memory problems uncomment this lines to free some memory
import gc
import torch
del foundation_model
del trainer
del train_sample
torch.cuda.empty_cache()
gc.collect()

"""## Step 7: Load the PEFT model and examine the change in output quality
Now that we've finetuned and saved our PEFT model, we're ready to load it and see if it has improved it haiku-writing abilities
"""

#import peft
from peft import AutoPeftModelForCausalLM, PeftConfig
#import os

device_map = {"": 0}
working_dir = 'sample_data'

output_directory = os.path.join(working_dir, "peft_lab_outputs")
peft_model_path = os.path.join(output_directory, f"lora_model")

bnb_config2 = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

#Load the Model.
loaded_model = AutoPeftModelForCausalLM.from_pretrained(
                                        peft_model_path,
                                        #torch_dtype=torch.bfloat16,
                                        is_trainable=False,
                                        #load_in_4bit=True,
                                        quantization_config=bnb_config2,
                                        device_map = 'cuda')

input_sentences = tokenizer("Write a haiku that describe nature.", return_tensors="pt").to('cuda')
foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=100)

print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))

"""Much better! No repititions and much more creative..."""